{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCuLREgZRDpHc1tJR5RwQv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/udupa-varun/pyimagesearch_uni/blob/main/deep_learning/103/backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3Lg9gtL8Ntr",
        "outputId": "dcb35bbe-b38e-4882-b8d7-ce995fa1f392"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-10 10:48:49--  https://pyimagesearch-code-downloads.s3-us-west-2.amazonaws.com/backpropagation-python/backpropagation-python.zip\n",
            "Resolving pyimagesearch-code-downloads.s3-us-west-2.amazonaws.com (pyimagesearch-code-downloads.s3-us-west-2.amazonaws.com)... 52.218.225.129, 52.92.224.162, 52.92.248.226, ...\n",
            "Connecting to pyimagesearch-code-downloads.s3-us-west-2.amazonaws.com (pyimagesearch-code-downloads.s3-us-west-2.amazonaws.com)|52.218.225.129|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20243 (20K) [binary/octet-stream]\n",
            "Saving to: ‘backpropagation-python.zip’\n",
            "\n",
            "backpropagation-pyt 100%[===================>]  19.77K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2023-02-10 10:48:50 (321 KB/s) - ‘backpropagation-python.zip’ saved [20243/20243]\n",
            "\n",
            "/content/backpropagation-python\n"
          ]
        }
      ],
      "source": [
        "!wget https://pyimagesearch-code-downloads.s3-us-west-2.amazonaws.com/backpropagation-python/backpropagation-python.zip\n",
        "!unzip -qq backpropagation-python.zip\n",
        "%cd backpropagation-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn import datasets\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "iBBa4TX48d4P"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, layers, alpha=0.1):\n",
        "        # init list of weights matrices,\n",
        "        # then store network architecture and learning rate\n",
        "        self.W = []\n",
        "        self.layers = layers\n",
        "        self.alpha = alpha\n",
        "\n",
        "        # start looping from index of first layer\n",
        "        # stop before reaching last two layers\n",
        "        for i in np.arange(0, len(layers) - 2):\n",
        "            # randomly init a weight matrix\n",
        "            # connecting number of nodes in each respective layer together\n",
        "            # add extra node for bias\n",
        "            w = np.random.randn(layers[i] + 1, layers[i + 1] + 1)\n",
        "            self.W.append(w / np.sqrt(layers[1]))\n",
        "\n",
        "        # last two layers are a special case\n",
        "        # input connections need a bias term but output does not\n",
        "        w = np.random.randn(layers[-2] + 1, layers[-1])\n",
        "        self.W.append(w / np.sqrt(layers[-2]))\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        # construct and return a string that represents the net arch\n",
        "        return \"NeuralNetwork: {}\".format(\"-\".join(str(l) for l in self.layers))\n",
        "\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        # compute sigmoid activation value for input\n",
        "        return 1.0 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "    def sigmoid_deriv(self, x):\n",
        "        # compute derivative of sigmoid function ASSUMING\n",
        "        # input x is the output sigmoid activation value\n",
        "        return x * (1 - x)\n",
        "\n",
        "\n",
        "    def fit(self, X, y, epochs=1000, display_update=100):\n",
        "        # insert a column of 1s as last entry in feature matrix\n",
        "        # bias trick\n",
        "        X = np.c_[X, np.ones((X.shape[0]))]\n",
        "\n",
        "        # loop over number of epochs\n",
        "        for epoch in np.arange(0, epochs):\n",
        "            # loop over data points and train network\n",
        "            for (x, target) in zip(X, y):\n",
        "                self.fit_partial(x, target)\n",
        "            \n",
        "            # check to see if we should display training update\n",
        "            if epoch == 0 or (epoch + 1) % display_update == 0:\n",
        "                loss = self.calculate_loss(X, y)\n",
        "                print(f\"[INFO] epoch={epoch + 1}, loss={loss:.7f}\")\n",
        "\n",
        "\n",
        "    def fit_partial(self, x, target):\n",
        "        # construct list of output activations for each layer,\n",
        "        # as our data point flows through the network\n",
        "        # first activation is a special case - it's the input vector itself\n",
        "        A = [np.atleast_2d(x)]\n",
        "\n",
        "        # FEEDFORWARD:\n",
        "        # loop over layers in network\n",
        "        for layer in np.arange(0, len(self.W)):\n",
        "            # feedforward activation at the current layer\n",
        "            # take dot product between activation and weight matrix\n",
        "            # \"net input\" to the current layer\n",
        "            net = A[layer].dot(self.W[layer])\n",
        "\n",
        "            # compute \"net output\" - apply activation function\n",
        "            out = self.sigmoid(net)\n",
        "\n",
        "            # add net output to list of activations\n",
        "            A.append(out)\n",
        "\n",
        "        # BACKPROPAGATION\n",
        "        # first phase of backpropagation\n",
        "        # compute difference between prediction (final out activation)\n",
        "        # and the true target value\n",
        "        error = A[-1] - target\n",
        "\n",
        "        # apply chain rule and build our list of deltas 'D'\n",
        "        # first entry is the error of output layer times derivative of\n",
        "        # activation function for the output value\n",
        "        D = [error * self.sigmoid_deriv(A[-1])]\n",
        "\n",
        "        # loop over layers in reverse order (ignoring last two, accounted)\n",
        "        for layer in np.arange(len(A) - 2, 0, -1):\n",
        "            # delta for current layer is equal to \n",
        "            # delta of previous layer dotted with weight matrix of current layer, \n",
        "            # followed by multiplying delta by deriv of activation func for\n",
        "            # activations of the current layer\n",
        "            delta = D[-1].dot(self.W[layer].T)\n",
        "            delta = delta * self.sigmoid_deriv(A[layer])\n",
        "            D.append(delta)\n",
        "\n",
        "        # since we looped over layers in reverse order, reverse deltas\n",
        "        D = D[::-1]\n",
        "\n",
        "        # WEIGHT UPDATE\n",
        "        # loop over the layers\n",
        "        for layer in np.arange(0, len(self.W)):\n",
        "            # update weights by taking dot prod of layer activations with\n",
        "            # their respective deltas, then multiplying this value with a small\n",
        "            # learning rate and adding to our weight matrix\n",
        "            self.W[layer] += -self.alpha * A[layer].T.dot(D[layer])\n",
        "\n",
        "    \n",
        "    def predict(self, X, add_bias=True):\n",
        "        # init output prediction as input features\n",
        "        # this value will be forward propagated to obtain final prediction\n",
        "        p = np.atleast_2d(X)\n",
        "\n",
        "        # check if bias column should be added\n",
        "        if add_bias:\n",
        "            # insert a column of 1s as last entry in feature matrix\n",
        "            # bias trick\n",
        "            p = np.c_[p, np.ones((p.shape[0]))]\n",
        "\n",
        "        # loop over layers in network\n",
        "        for layer in np.arange(0, len(self.W)):\n",
        "            # compute output prediction\n",
        "            # take dot prod of current activation value p and \n",
        "            # weight matrix for current layer, then pass through activation\n",
        "            p = self.sigmoid(np.dot(p, self.W[layer]))\n",
        "\n",
        "        # return predicted value\n",
        "        return p\n",
        "\n",
        "\n",
        "    def calculate_loss(self, X, targets):\n",
        "        # make predictions for the input data points, then compute loss\n",
        "        targets = np.atleast_2d(targets)\n",
        "        predictions = self.predict(X, add_bias=False)\n",
        "        loss = 0.5 * np.sum((predictions - targets) ** 2)\n",
        "\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "UAJc9Bu287x7"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# construct the XOR dataset\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])"
      ],
      "metadata": {
        "id": "7VoMHZFZEDit"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define our 2-2-1 NN and train it\n",
        "nn = NeuralNetwork([2, 2, 1], alpha=0.5)\n",
        "nn.fit(X, y, epochs=20000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqVDZyHUELAI",
        "outputId": "2f71e475-347d-46e4-a7e8-f86fed618970"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] epoch=1, loss=0.5033469\n",
            "[INFO] epoch=100, loss=0.4988608\n",
            "[INFO] epoch=200, loss=0.4946892\n",
            "[INFO] epoch=300, loss=0.4818631\n",
            "[INFO] epoch=400, loss=0.4500477\n",
            "[INFO] epoch=500, loss=0.3997506\n",
            "[INFO] epoch=600, loss=0.3463599\n",
            "[INFO] epoch=700, loss=0.2954147\n",
            "[INFO] epoch=800, loss=0.2482711\n",
            "[INFO] epoch=900, loss=0.2125601\n",
            "[INFO] epoch=1000, loss=0.1884188\n",
            "[INFO] epoch=1100, loss=0.1724714\n",
            "[INFO] epoch=1200, loss=0.1614985\n",
            "[INFO] epoch=1300, loss=0.1531116\n",
            "[INFO] epoch=1400, loss=0.1441845\n",
            "[INFO] epoch=1500, loss=0.0912863\n",
            "[INFO] epoch=1600, loss=0.0257524\n",
            "[INFO] epoch=1700, loss=0.0172918\n",
            "[INFO] epoch=1800, loss=0.0134560\n",
            "[INFO] epoch=1900, loss=0.0110945\n",
            "[INFO] epoch=2000, loss=0.0094531\n",
            "[INFO] epoch=2100, loss=0.0082348\n",
            "[INFO] epoch=2200, loss=0.0072913\n",
            "[INFO] epoch=2300, loss=0.0065378\n",
            "[INFO] epoch=2400, loss=0.0059220\n",
            "[INFO] epoch=2500, loss=0.0054091\n",
            "[INFO] epoch=2600, loss=0.0049754\n",
            "[INFO] epoch=2700, loss=0.0046041\n",
            "[INFO] epoch=2800, loss=0.0042825\n",
            "[INFO] epoch=2900, loss=0.0040015\n",
            "[INFO] epoch=3000, loss=0.0037538\n",
            "[INFO] epoch=3100, loss=0.0035339\n",
            "[INFO] epoch=3200, loss=0.0033375\n",
            "[INFO] epoch=3300, loss=0.0031610\n",
            "[INFO] epoch=3400, loss=0.0030016\n",
            "[INFO] epoch=3500, loss=0.0028570\n",
            "[INFO] epoch=3600, loss=0.0027251\n",
            "[INFO] epoch=3700, loss=0.0026044\n",
            "[INFO] epoch=3800, loss=0.0024936\n",
            "[INFO] epoch=3900, loss=0.0023915\n",
            "[INFO] epoch=4000, loss=0.0022971\n",
            "[INFO] epoch=4100, loss=0.0022097\n",
            "[INFO] epoch=4200, loss=0.0021284\n",
            "[INFO] epoch=4300, loss=0.0020527\n",
            "[INFO] epoch=4400, loss=0.0019820\n",
            "[INFO] epoch=4500, loss=0.0019158\n",
            "[INFO] epoch=4600, loss=0.0018538\n",
            "[INFO] epoch=4700, loss=0.0017955\n",
            "[INFO] epoch=4800, loss=0.0017406\n",
            "[INFO] epoch=4900, loss=0.0016889\n",
            "[INFO] epoch=5000, loss=0.0016400\n",
            "[INFO] epoch=5100, loss=0.0015938\n",
            "[INFO] epoch=5200, loss=0.0015501\n",
            "[INFO] epoch=5300, loss=0.0015086\n",
            "[INFO] epoch=5400, loss=0.0014692\n",
            "[INFO] epoch=5500, loss=0.0014317\n",
            "[INFO] epoch=5600, loss=0.0013961\n",
            "[INFO] epoch=5700, loss=0.0013621\n",
            "[INFO] epoch=5800, loss=0.0013296\n",
            "[INFO] epoch=5900, loss=0.0012987\n",
            "[INFO] epoch=6000, loss=0.0012691\n",
            "[INFO] epoch=6100, loss=0.0012407\n",
            "[INFO] epoch=6200, loss=0.0012136\n",
            "[INFO] epoch=6300, loss=0.0011876\n",
            "[INFO] epoch=6400, loss=0.0011626\n",
            "[INFO] epoch=6500, loss=0.0011387\n",
            "[INFO] epoch=6600, loss=0.0011157\n",
            "[INFO] epoch=6700, loss=0.0010935\n",
            "[INFO] epoch=6800, loss=0.0010722\n",
            "[INFO] epoch=6900, loss=0.0010517\n",
            "[INFO] epoch=7000, loss=0.0010319\n",
            "[INFO] epoch=7100, loss=0.0010129\n",
            "[INFO] epoch=7200, loss=0.0009945\n",
            "[INFO] epoch=7300, loss=0.0009767\n",
            "[INFO] epoch=7400, loss=0.0009596\n",
            "[INFO] epoch=7500, loss=0.0009430\n",
            "[INFO] epoch=7600, loss=0.0009269\n",
            "[INFO] epoch=7700, loss=0.0009114\n",
            "[INFO] epoch=7800, loss=0.0008964\n",
            "[INFO] epoch=7900, loss=0.0008819\n",
            "[INFO] epoch=8000, loss=0.0008678\n",
            "[INFO] epoch=8100, loss=0.0008541\n",
            "[INFO] epoch=8200, loss=0.0008409\n",
            "[INFO] epoch=8300, loss=0.0008280\n",
            "[INFO] epoch=8400, loss=0.0008155\n",
            "[INFO] epoch=8500, loss=0.0008034\n",
            "[INFO] epoch=8600, loss=0.0007916\n",
            "[INFO] epoch=8700, loss=0.0007802\n",
            "[INFO] epoch=8800, loss=0.0007690\n",
            "[INFO] epoch=8900, loss=0.0007582\n",
            "[INFO] epoch=9000, loss=0.0007477\n",
            "[INFO] epoch=9100, loss=0.0007374\n",
            "[INFO] epoch=9200, loss=0.0007274\n",
            "[INFO] epoch=9300, loss=0.0007177\n",
            "[INFO] epoch=9400, loss=0.0007082\n",
            "[INFO] epoch=9500, loss=0.0006990\n",
            "[INFO] epoch=9600, loss=0.0006900\n",
            "[INFO] epoch=9700, loss=0.0006812\n",
            "[INFO] epoch=9800, loss=0.0006726\n",
            "[INFO] epoch=9900, loss=0.0006643\n",
            "[INFO] epoch=10000, loss=0.0006561\n",
            "[INFO] epoch=10100, loss=0.0006481\n",
            "[INFO] epoch=10200, loss=0.0006404\n",
            "[INFO] epoch=10300, loss=0.0006328\n",
            "[INFO] epoch=10400, loss=0.0006253\n",
            "[INFO] epoch=10500, loss=0.0006181\n",
            "[INFO] epoch=10600, loss=0.0006110\n",
            "[INFO] epoch=10700, loss=0.0006040\n",
            "[INFO] epoch=10800, loss=0.0005972\n",
            "[INFO] epoch=10900, loss=0.0005906\n",
            "[INFO] epoch=11000, loss=0.0005841\n",
            "[INFO] epoch=11100, loss=0.0005777\n",
            "[INFO] epoch=11200, loss=0.0005715\n",
            "[INFO] epoch=11300, loss=0.0005654\n",
            "[INFO] epoch=11400, loss=0.0005594\n",
            "[INFO] epoch=11500, loss=0.0005536\n",
            "[INFO] epoch=11600, loss=0.0005478\n",
            "[INFO] epoch=11700, loss=0.0005422\n",
            "[INFO] epoch=11800, loss=0.0005367\n",
            "[INFO] epoch=11900, loss=0.0005313\n",
            "[INFO] epoch=12000, loss=0.0005260\n",
            "[INFO] epoch=12100, loss=0.0005208\n",
            "[INFO] epoch=12200, loss=0.0005157\n",
            "[INFO] epoch=12300, loss=0.0005107\n",
            "[INFO] epoch=12400, loss=0.0005058\n",
            "[INFO] epoch=12500, loss=0.0005010\n",
            "[INFO] epoch=12600, loss=0.0004963\n",
            "[INFO] epoch=12700, loss=0.0004916\n",
            "[INFO] epoch=12800, loss=0.0004871\n",
            "[INFO] epoch=12900, loss=0.0004826\n",
            "[INFO] epoch=13000, loss=0.0004782\n",
            "[INFO] epoch=13100, loss=0.0004739\n",
            "[INFO] epoch=13200, loss=0.0004697\n",
            "[INFO] epoch=13300, loss=0.0004655\n",
            "[INFO] epoch=13400, loss=0.0004614\n",
            "[INFO] epoch=13500, loss=0.0004574\n",
            "[INFO] epoch=13600, loss=0.0004534\n",
            "[INFO] epoch=13700, loss=0.0004495\n",
            "[INFO] epoch=13800, loss=0.0004457\n",
            "[INFO] epoch=13900, loss=0.0004419\n",
            "[INFO] epoch=14000, loss=0.0004382\n",
            "[INFO] epoch=14100, loss=0.0004346\n",
            "[INFO] epoch=14200, loss=0.0004310\n",
            "[INFO] epoch=14300, loss=0.0004275\n",
            "[INFO] epoch=14400, loss=0.0004240\n",
            "[INFO] epoch=14500, loss=0.0004206\n",
            "[INFO] epoch=14600, loss=0.0004172\n",
            "[INFO] epoch=14700, loss=0.0004139\n",
            "[INFO] epoch=14800, loss=0.0004106\n",
            "[INFO] epoch=14900, loss=0.0004074\n",
            "[INFO] epoch=15000, loss=0.0004043\n",
            "[INFO] epoch=15100, loss=0.0004012\n",
            "[INFO] epoch=15200, loss=0.0003981\n",
            "[INFO] epoch=15300, loss=0.0003951\n",
            "[INFO] epoch=15400, loss=0.0003921\n",
            "[INFO] epoch=15500, loss=0.0003892\n",
            "[INFO] epoch=15600, loss=0.0003863\n",
            "[INFO] epoch=15700, loss=0.0003834\n",
            "[INFO] epoch=15800, loss=0.0003806\n",
            "[INFO] epoch=15900, loss=0.0003778\n",
            "[INFO] epoch=16000, loss=0.0003751\n",
            "[INFO] epoch=16100, loss=0.0003724\n",
            "[INFO] epoch=16200, loss=0.0003698\n",
            "[INFO] epoch=16300, loss=0.0003671\n",
            "[INFO] epoch=16400, loss=0.0003646\n",
            "[INFO] epoch=16500, loss=0.0003620\n",
            "[INFO] epoch=16600, loss=0.0003595\n",
            "[INFO] epoch=16700, loss=0.0003570\n",
            "[INFO] epoch=16800, loss=0.0003546\n",
            "[INFO] epoch=16900, loss=0.0003522\n",
            "[INFO] epoch=17000, loss=0.0003498\n",
            "[INFO] epoch=17100, loss=0.0003474\n",
            "[INFO] epoch=17200, loss=0.0003451\n",
            "[INFO] epoch=17300, loss=0.0003428\n",
            "[INFO] epoch=17400, loss=0.0003406\n",
            "[INFO] epoch=17500, loss=0.0003383\n",
            "[INFO] epoch=17600, loss=0.0003361\n",
            "[INFO] epoch=17700, loss=0.0003340\n",
            "[INFO] epoch=17800, loss=0.0003318\n",
            "[INFO] epoch=17900, loss=0.0003297\n",
            "[INFO] epoch=18000, loss=0.0003276\n",
            "[INFO] epoch=18100, loss=0.0003255\n",
            "[INFO] epoch=18200, loss=0.0003235\n",
            "[INFO] epoch=18300, loss=0.0003215\n",
            "[INFO] epoch=18400, loss=0.0003195\n",
            "[INFO] epoch=18500, loss=0.0003175\n",
            "[INFO] epoch=18600, loss=0.0003156\n",
            "[INFO] epoch=18700, loss=0.0003136\n",
            "[INFO] epoch=18800, loss=0.0003117\n",
            "[INFO] epoch=18900, loss=0.0003099\n",
            "[INFO] epoch=19000, loss=0.0003080\n",
            "[INFO] epoch=19100, loss=0.0003062\n",
            "[INFO] epoch=19200, loss=0.0003044\n",
            "[INFO] epoch=19300, loss=0.0003026\n",
            "[INFO] epoch=19400, loss=0.0003008\n",
            "[INFO] epoch=19500, loss=0.0002991\n",
            "[INFO] epoch=19600, loss=0.0002973\n",
            "[INFO] epoch=19700, loss=0.0002956\n",
            "[INFO] epoch=19800, loss=0.0002939\n",
            "[INFO] epoch=19900, loss=0.0002922\n",
            "[INFO] epoch=20000, loss=0.0002906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loop over XOR data points and evaluate trained NN\n",
        "for (x, target) in zip(X, y):\n",
        "    # make a prediction and display result\n",
        "    pred = nn.predict(x)[0][0]\n",
        "    step = 1 if pred > 0.5 else 0\n",
        "    print(f\"[INFO] data={x}, ground-truth={target[0]}, pred={pred:.4f}, step={step}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfQ6B4GUEU6p",
        "outputId": "5991d925-2324-4eb6-c1b7-9f2420c050dc"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] data=[0 0], ground-truth=0, pred=0.0107, step=0\n",
            "[INFO] data=[0 1], ground-truth=1, pred=0.9888, step=1\n",
            "[INFO] data=[1 0], ground-truth=1, pred=0.9884, step=1\n",
            "[INFO] data=[1 1], ground-truth=0, pred=0.0144, step=0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MNIST Sample"
      ],
      "metadata": {
        "id": "7NboRdQqGkb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load MNIST dataset\n",
        "# apply min/max scaling to scale pixel intensities to [0, 1]\n",
        "# each image is represented by an 8x8=64-dim feature vector\n",
        "print(\"[INFO] loading MNIST (sample) dataset...\")\n",
        "digits = datasets.load_digits()\n",
        "data = digits.data.astype(\"float\")\n",
        "data = (data - data.min()) / (data.max() - data.min())\n",
        "\n",
        "print(f\"[INFO] samples: {data.shape[0]}, dim: {data.shape[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpk1uwnMGtzM",
        "outputId": "0eb724d5-5058-4c7c-8255-7d95ac6b845f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] loading MNIST (sample) dataset...\n",
            "[INFO] samples: 1797, dim: 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# construct training and testing splits\n",
        "(train_x, test_x, train_y, test_y) = train_test_split(\n",
        "    data, digits.target, test_size=0.25\n",
        ")\n",
        "\n",
        "# convert labels from integers to vectors\n",
        "train_y = LabelBinarizer().fit_transform(train_y)\n",
        "test_y = LabelBinarizer().fit_transform(test_y)"
      ],
      "metadata": {
        "id": "lbqtHhF8KPUs"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train network\n",
        "print(\"[INFO] training network...\")\n",
        "nn = NeuralNetwork([train_x.shape[1], 32, 16, 10])\n",
        "print(f\"[INFO] {nn}\")\n",
        "nn.fit(train_x, train_y, epochs=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rdFNLDRKnnL",
        "outputId": "4be2fa9b-de67-414d-a121-ebad1170c671"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] training network...\n",
            "[INFO] NeuralNetwork: 64-32-16-10\n",
            "[INFO] epoch=1, loss=606.2714559\n",
            "[INFO] epoch=100, loss=5.2762401\n",
            "[INFO] epoch=200, loss=1.5183176\n",
            "[INFO] epoch=300, loss=1.0435741\n",
            "[INFO] epoch=400, loss=0.8643565\n",
            "[INFO] epoch=500, loss=0.7716486\n",
            "[INFO] epoch=600, loss=0.7154595\n",
            "[INFO] epoch=700, loss=0.6779529\n",
            "[INFO] epoch=800, loss=0.6512295\n",
            "[INFO] epoch=900, loss=0.6312708\n",
            "[INFO] epoch=1000, loss=0.6158239\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate network\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predictions = nn.predict(test_x)\n",
        "predictions = predictions.argmax(axis=1)\n",
        "print(classification_report(test_y.argmax(axis=1), predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnAR6jjxK6gn",
        "outputId": "725aeb7d-00d4-4149-ec56-c62e2dbab130"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99        45\n",
            "           1       1.00      0.98      0.99        48\n",
            "           2       0.98      1.00      0.99        48\n",
            "           3       1.00      1.00      1.00        43\n",
            "           4       1.00      0.93      0.96        40\n",
            "           5       0.95      0.98      0.97        43\n",
            "           6       1.00      1.00      1.00        48\n",
            "           7       1.00      1.00      1.00        40\n",
            "           8       0.98      0.98      0.98        50\n",
            "           9       0.90      0.96      0.92        45\n",
            "\n",
            "    accuracy                           0.98       450\n",
            "   macro avg       0.98      0.98      0.98       450\n",
            "weighted avg       0.98      0.98      0.98       450\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NzfGmz1GLcUE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}