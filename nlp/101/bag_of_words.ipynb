{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFWPAYGus1BNfsLg2p4mEJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/udupa-varun/pyimagesearch_uni/blob/main/nlp/101/bag_of_words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emfR9nJmyQYy",
        "outputId": "610a4d5e-4971-4fd5-b073-220d34e69801"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-12 22:27:22--  https://pyimagesearch-code-downloads.s3-us-west-2.amazonaws.com/bag-of-word/bag-of-word.zip\n",
            "Resolving pyimagesearch-code-downloads.s3-us-west-2.amazonaws.com (pyimagesearch-code-downloads.s3-us-west-2.amazonaws.com)... 52.218.229.105, 3.5.81.129, 52.92.248.122, ...\n",
            "Connecting to pyimagesearch-code-downloads.s3-us-west-2.amazonaws.com (pyimagesearch-code-downloads.s3-us-west-2.amazonaws.com)|52.218.229.105|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 88412 (86K) [binary/octet-stream]\n",
            "Saving to: ‘bag-of-word.zip’\n",
            "\n",
            "bag-of-word.zip     100%[===================>]  86.34K  --.-KB/s    in 0.09s   \n",
            "\n",
            "2024-01-12 22:27:23 (1005 KB/s) - ‘bag-of-word.zip’ saved [88412/88412]\n",
            "\n",
            "/content/bag-of-word\n"
          ]
        }
      ],
      "source": [
        "!wget https://pyimagesearch-code-downloads.s3-us-west-2.amazonaws.com/bag-of-word/bag-of-word.zip\n",
        "!unzip -qq bag-of-word.zip\n",
        "%cd bag-of-word"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "9-iVXGOsyWHm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuration of the architecture"
      ],
      "metadata": {
        "id": "5aIMP5nizIwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    # data to be used\n",
        "    data_dict = {\n",
        "        \"sentence\": [\n",
        "            \"Avengers is a great movie.\",\n",
        "            \"I love Avengers it is great.\",\n",
        "            \"Avengers is a bad movie.\",\n",
        "            \"I hate Avengers.\",\n",
        "            \"I didnt like the Avengers movie.\",\n",
        "            \"I think Avengers is a bad movie.\",\n",
        "            \"I love the movie.\",\n",
        "            \"I think it is great.\"\n",
        "        ],\n",
        "        \"sentiment\": [\n",
        "            \"good\",\n",
        "            \"good\",\n",
        "            \"bad\",\n",
        "            \"bad\",\n",
        "            \"bad\",\n",
        "            \"bad\",\n",
        "            \"good\",\n",
        "            \"good\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # list of stopwords\n",
        "    stop_words = [\"is\", \"a\", \"i\", \"it\"]\n",
        "\n",
        "    # define model training parameters\n",
        "    epochs = 30\n",
        "    batch_size = 10\n",
        "\n",
        "    # define number of dense units\n",
        "    dense_units = 50\n",
        "\n",
        "config = Config()"
      ],
      "metadata": {
        "id": "Z2N8jOINzNNe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess data"
      ],
      "metadata": {
        "id": "9QJi0b6H14Pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(sent_df, stop_words, key=\"sentence\"):\n",
        "    # loop over sentences\n",
        "    for num in range(len(sent_df[key])):\n",
        "        sentence = sent_df[key][num]\n",
        "        sentence = re.sub(\n",
        "            r\"[^a-zA-z0-9]\", \" \", sentence.lower()\n",
        "        ).split()\n",
        "\n",
        "        # define a list for processed words\n",
        "        new_words = list()\n",
        "\n",
        "        # loop over words in each sentence\n",
        "        # filter out the stop words\n",
        "        for word in sentence:\n",
        "            if word not in stop_words:\n",
        "                new_words.append(word)\n",
        "\n",
        "        # replace sentence with list of new words\n",
        "        sent_df[key][num] = new_words\n",
        "\n",
        "    return sent_df\n",
        "\n",
        "\n",
        "def prepare_tokenizer(df, sent_key=\"sentence\", output_key=\"sentiment\"):\n",
        "    # counters for tokenizer indices\n",
        "    word_counter = 0\n",
        "    label_counter = 0\n",
        "\n",
        "    # placeholder for tokenizer\n",
        "    text_dict = dict()\n",
        "    label_dict = dict()\n",
        "\n",
        "    # loop over the sentences\n",
        "    for entry in df[sent_key]:\n",
        "        # loop over each word and check if encountered before\n",
        "        for word in entry:\n",
        "            if word not in text_dict.keys():\n",
        "                text_dict[word] = word_counter\n",
        "                word_counter += 1\n",
        "\n",
        "    # repeat for labels\n",
        "    for label in df[output_key]:\n",
        "        if label not in label_dict.keys():\n",
        "            label_dict[label] = label_counter\n",
        "            label_counter += 1\n",
        "\n",
        "    return (text_dict, label_dict)"
      ],
      "metadata": {
        "id": "eSfaJssb175y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to calculate bag of words"
      ],
      "metadata": {
        "id": "3jgHbIaU4CT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_bag_of_words(text, sentence):\n",
        "    # create a dict for frequency check\n",
        "    freq_dict = dict.fromkeys(text, 0)\n",
        "\n",
        "    # loop over words in sentences\n",
        "    for word in sentence:\n",
        "        freq_dict[word] = sentence.count(word)\n",
        "\n",
        "    return freq_dict"
      ],
      "metadata": {
        "id": "3VdSHzn_4GCH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build bag of words model"
      ],
      "metadata": {
        "id": "s8Z1ZGwy4lR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_shallow_net():\n",
        "    # define model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(config.dense_units, input_dim=10, activation=\"relu\"))\n",
        "    model.add(Dense(config.dense_units, activation=\"relu\"))\n",
        "    model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "    # compile model\n",
        "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "r7rJTHVx4o0v"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build tensorflow wrapper"
      ],
      "metadata": {
        "id": "WfSegWIH5Y_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tensorflow_wrap(df):\n",
        "    # create tokenizerr for sentences\n",
        "    tokenizer_sentence = Tokenizer()\n",
        "\n",
        "    # create tokenizer for labels\n",
        "    tokenizer_labels = Tokenizer()\n",
        "\n",
        "    # fit tokenizer on documents\n",
        "    tokenizer_sentence.fit_on_texts(df[\"sentence\"])\n",
        "\n",
        "    # fit tokenizer on labels\n",
        "    tokenizer_labels.fit_on_texts(df[\"sentiment\"])\n",
        "\n",
        "    # create vectors using tensorflow\n",
        "    encoded_data = tokenizer_sentence.texts_to_matrix(\n",
        "        texts=df[\"sentence\"], mode=\"count\"\n",
        "    )\n",
        "\n",
        "    # add label column\n",
        "    labels = df[\"sentiment\"]\n",
        "\n",
        "    # correct label vectors\n",
        "    for i in range(len(labels)):\n",
        "        labels[i] = tokenizer_labels.word_index[labels[i]] - 1\n",
        "\n",
        "    # return data and labels\n",
        "    return (encoded_data[:, 1:], labels.astype(\"float32\"))"
      ],
      "metadata": {
        "id": "Xrq-S4z25fz7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the models"
      ],
      "metadata": {
        "id": "jZziwHpd6g5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert input data dict to a pandas df\n",
        "df = pd.DataFrame.from_dict(config.data_dict)\n",
        "\n",
        "# preprocess data frame and create data dicts\n",
        "preprocessed_df = preprocess(sent_df=df, stop_words=config.stop_words)\n",
        "(text_dict, label_dict) = prepare_tokenizer(df)\n",
        "\n",
        "# init vector list\n",
        "freq_list = list()\n",
        "\n",
        "# build vectors from sentences\n",
        "for sentence in df[\"sentence\"]:\n",
        "    # create entries for each sentence and update vector list\n",
        "    entry_freq = calculate_bag_of_words(text=text_dict, sentence=sentence)\n",
        "    freq_list.append(entry_freq)\n",
        "\n",
        "# create empty df for vectors\n",
        "final_df = pd.DataFrame()\n",
        "\n",
        "# loop over vectors and concat them\n",
        "for vector in freq_list:\n",
        "    vector = pd.DataFrame([vector])\n",
        "    final_df = pd.concat([final_df, vector], ignore_index=True)\n",
        "\n",
        "# add label column to final data frame\n",
        "final_df[\"label\"] = df[\"sentiment\"]\n",
        "\n",
        "# convert label to corresponding vector\n",
        "for i in range(len(final_df[\"label\"])):\n",
        "    final_df[\"label\"][i] = label_dict[final_df[\"label\"][i]]\n",
        "\n",
        "# init vanilla model\n",
        "print(\"[INFO] Compiling model...\")\n",
        "shallow_model = build_shallow_net()\n",
        "\n",
        "# fit keras model on dataset\n",
        "shallow_model.fit(\n",
        "    final_df.iloc[:, 0:10],\n",
        "    final_df.iloc[:, 10].astype(\"float32\"),\n",
        "    epochs=config.epochs,\n",
        "    batch_size=config.batch_size\n",
        ")\n",
        "\n",
        "# create dataset using TF\n",
        "train_x, train_y = tensorflow_wrap(df)\n",
        "\n",
        "# init new model\n",
        "print(\"[INFO] Compiling model with TF wrapped data...\")\n",
        "tf_model = build_shallow_net()\n",
        "\n",
        "# fit keras model on TF dataset\n",
        "tf_model.fit(\n",
        "    train_x,\n",
        "    train_y,\n",
        "    epochs=config.epochs,\n",
        "    batch_size=config.batch_size\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjiCBK-N6kcz",
        "outputId": "2f154c98-adef-4b60-991e-bceef495c0dd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-f0ab6798dc03>:30: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  final_df[\"label\"][i] = label_dict[final_df[\"label\"][i]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Compiling model...\n",
            "Epoch 1/30\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6732 - accuracy: 0.6250\n",
            "Epoch 2/30\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6634 - accuracy: 0.6250\n",
            "Epoch 3/30\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6539 - accuracy: 0.6250\n",
            "Epoch 4/30\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6444 - accuracy: 0.6250\n",
            "Epoch 5/30\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6352 - accuracy: 0.6250\n",
            "Epoch 6/30\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6261 - accuracy: 0.6250\n",
            "Epoch 7/30\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6169 - accuracy: 0.7500\n",
            "Epoch 8/30\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6077 - accuracy: 0.7500\n",
            "Epoch 9/30\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5985 - accuracy: 0.7500\n",
            "Epoch 10/30\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.5893 - accuracy: 0.7500\n",
            "Epoch 11/30\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5801 - accuracy: 0.7500\n",
            "Epoch 12/30\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5710 - accuracy: 0.7500\n",
            "Epoch 13/30\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5620 - accuracy: 0.7500\n",
            "Epoch 14/30\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.5529 - accuracy: 0.8750\n",
            "Epoch 15/30\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.5442 - accuracy: 0.8750\n",
            "Epoch 16/30\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5354 - accuracy: 0.8750\n",
            "Epoch 17/30\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5268 - accuracy: 0.8750\n",
            "Epoch 18/30\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.5180 - accuracy: 1.0000\n",
            "Epoch 19/30\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.5092 - accuracy: 1.0000\n",
            "Epoch 20/30\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.5003 - accuracy: 1.0000\n",
            "Epoch 21/30\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4915 - accuracy: 1.0000\n",
            "Epoch 22/30\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.4826 - accuracy: 1.0000\n",
            "Epoch 23/30\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.4736 - accuracy: 1.0000\n",
            "Epoch 24/30\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.4646 - accuracy: 1.0000\n",
            "Epoch 25/30\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.4555 - accuracy: 1.0000\n",
            "Epoch 26/30\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.4462 - accuracy: 1.0000\n",
            "Epoch 27/30\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4369 - accuracy: 1.0000\n",
            "Epoch 28/30\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.4276 - accuracy: 1.0000\n",
            "Epoch 29/30\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.4182 - accuracy: 1.0000\n",
            "Epoch 30/30\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4089 - accuracy: 1.0000\n",
            "[INFO] Compiling model with TF wrapped data...\n",
            "Epoch 1/30\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6869 - accuracy: 0.5000\n",
            "Epoch 2/30\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6771 - accuracy: 0.5000\n",
            "Epoch 3/30\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6677 - accuracy: 0.5000\n",
            "Epoch 4/30\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6587 - accuracy: 0.5000\n",
            "Epoch 5/30\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6499 - accuracy: 0.5000\n",
            "Epoch 6/30\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6411 - accuracy: 0.6250\n",
            "Epoch 7/30\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.6325 - accuracy: 0.6250\n",
            "Epoch 8/30\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6240 - accuracy: 0.6250\n",
            "Epoch 9/30\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6158 - accuracy: 0.6250\n",
            "Epoch 10/30\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6079 - accuracy: 0.7500\n",
            "Epoch 11/30\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.6001 - accuracy: 0.7500\n",
            "Epoch 12/30\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5926 - accuracy: 0.7500\n",
            "Epoch 13/30\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5853 - accuracy: 0.7500\n",
            "Epoch 14/30\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5778 - accuracy: 0.7500\n",
            "Epoch 15/30\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.5705 - accuracy: 0.8750\n",
            "Epoch 16/30\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.5632 - accuracy: 0.8750\n",
            "Epoch 17/30\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.5559 - accuracy: 0.8750\n",
            "Epoch 18/30\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.5487 - accuracy: 0.8750\n",
            "Epoch 19/30\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.5418 - accuracy: 0.8750\n",
            "Epoch 20/30\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.5348 - accuracy: 0.8750\n",
            "Epoch 21/30\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.5277 - accuracy: 0.8750\n",
            "Epoch 22/30\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5205 - accuracy: 0.8750\n",
            "Epoch 23/30\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.5132 - accuracy: 0.8750\n",
            "Epoch 24/30\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.5056 - accuracy: 0.8750\n",
            "Epoch 25/30\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.4980 - accuracy: 0.8750\n",
            "Epoch 26/30\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4902 - accuracy: 0.8750\n",
            "Epoch 27/30\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4825 - accuracy: 0.8750\n",
            "Epoch 28/30\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.4746 - accuracy: 0.8750\n",
            "Epoch 29/30\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.4666 - accuracy: 0.8750\n",
            "Epoch 30/30\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.4585 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e25c1db2a10>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wif4EKaq9F-8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}